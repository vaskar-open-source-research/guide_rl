- algorithm.adv_estimator=rloo
- data.train_files=["experiments/math_reasoning/data/ossmall_45K_r1_prompt/train.parquet"]
- data.val_files=["experiments/math_reasoning/data/ossmall_45K_r1_prompt/validation.parquet"]
- data.test_files=["experiments/math_reasoning/data/ossmall_45K_r1_prompt/test.parquet"]
- data.train_batch_size=8
- data.val_batch_size=128
- data.max_prompt_length=1024
- data.max_response_length=512
- data.apply_chat_template=False
- actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B
- actor_rollout_ref.actor.optim.lr=5e-7
- actor_rollout_ref.actor.ppo_mini_batch_size=8
- actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1
- actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1
- actor_rollout_ref.rollout.tensor_model_parallel_size=1
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.n=4
- actor_rollout_ref.rollout.gpu_memory_utilization=0.7
- actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- actor_rollout_ref.actor.entropy_coeff=0.0
- algorithm.kl_ctrl.kl_coef=0.0
- actor_rollout_ref.actor.use_kl_loss=True
- actor_rollout_ref.actor.kl_loss_coef=0.0
- trainer.critic_warmup=0
- trainer.logger=['console','wandb']
- trainer.project_name=verl-dev
- trainer.experiment_name=Qwen-0.5-save-test-v3
- trainer.default_local_dir=./ckpt
- trainer.s3_path="s3://scale-ml/genai/maple/reasoner_models/Qwen-0.5-save-test-v3"
- trainer.n_gpus_per_node=8
- trainer.nnodes=1
- trainer.save_freq=1
- trainer.test_freq=16
- trainer.total_epochs=100
- scale_reasoning.response_format=reasoning_format
- reward_model.model_type=prime
- reward_model.mini_batch_size=8
- reward_model.rm_coef=5
- reward_model.prime_model.path=Qwen/Qwen2.5-0.5B
- reward_model.prime_model.ref_path=Qwen/Qwen2.5-0.5B
- reward_model.model.input_tokenizer=null
- reward_model.prime_model.use_remove_padding=True
- reward_model.prime_model.granularity=token
- reward_model.micro_batch_size_per_gpu=1
- reward_model.prime_model.update=after
- reward_model.prime_model.beta_train=0.05
- reward_model.prime_model.optim.lr=1e-6
- reward_model.prime_model.optim.grad_clip=10.0
- reward_model.prime_model.input_tokenizer=null
- reward_model.enable=True
- trainer.val_before_train=False
- data.filter_accuracy=False
- data.filter_truncated=False
- data.accuracy_lower_bound=0.001
- data.accuracy_upper_bound=0.999
- scale_reasoning.do_verify_format=True
- actor_rollout_ref.rollout.max_num_batched_tokens=2048
